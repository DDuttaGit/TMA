# Towards Mimicking Attention

### _Anonymous submission_
<p align="center">
<img src="https://github.com/user-attachments/assets/1ed3d235-7528-4f08-98e2-7a248f89bf77" />
</p>

Construction of the transformer network $\mathcal{U}$ (see the diagram above) has been algorithmically achieved using Restricted Access Sequence Programming [^1]. Realization of such a conceptual representation of $\mathcal{U}$ can be implemented using `Tracr` [^2].

Implementation of fundamental matrix operations and activations has been attached in this repository.


#### References
[^1]: Weiss, Gail, Yoav Goldberg, and Eran Yahav. "Thinking Like Transformers." In International Conference on Machine Learning, pp. 11080-11090. PMLR, 2021.
[^2]: Lindner, David, János Kramár, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. "Tracr: Compiled transformers as a laboratory for interpretability." Advances in Neural Information Processing Systems 36 (2023): 37876-37899.
